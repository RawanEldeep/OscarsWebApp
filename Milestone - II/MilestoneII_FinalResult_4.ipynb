{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjtX4Y-oS6J4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from humanize import number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyQ1qV-TGvn"
      },
      "source": [
        "# Functions and Intializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJXSDE8d0bU1"
      },
      "outputs": [],
      "source": [
        "#Intializing Global Lists\n",
        "AllPersons = []\n",
        "WorksAt = []\n",
        "Movies = []\n",
        "MovieProdComp = []\n",
        "Nomination = []\n",
        "AcademyAwards = []\n",
        "AcademyAwardsHost = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuMWG8qa4vFK"
      },
      "outputs": [],
      "source": [
        "#Intializing the Roles table\n",
        "Roles =  {\n",
        "  \"Directed by\" : \"Director\",\n",
        "  \"Produced by\" : \"Producer\",\n",
        "  \"Written by\" : \"Writter\",\n",
        "  \"Starring\" : \"Actor\",\n",
        "  \"Screenplay by\" : \"Screenwriter\",\n",
        "  \"Cinematography\" : \"Cinematographer\",\n",
        "  \"Music by\": \"Composer\"\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmZR4uhnTYXV"
      },
      "source": [
        "## Date Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqqTUCA3jIs8"
      },
      "outputs": [],
      "source": [
        "def ExtractDate(text):\n",
        "  #Cleaning the text first\n",
        "\n",
        "  text = re.sub(r'\\(.*?\\)', '', text)\n",
        "  text = re.sub(r'\\[.*?\\]', '', text)\n",
        "\n",
        "  # Putting the different patterns I observed\n",
        "  DatePatterns = [\n",
        "      r'\\d{4}-\\d{1,2}-\\d{1,2}',\n",
        "      r'\\w+ \\d{1,2}, \\d{4}',\n",
        "      r'\\d{1,2} \\w+ \\d{4}',\n",
        "      r'\\w+ \\d{4}',\n",
        "      r'\\d{4}'\n",
        "\n",
        "  ]\n",
        "\n",
        "  # Looking if the text passed matches any of the patterns I observed\n",
        "  for date in DatePatterns:\n",
        "    Dt = re.search(date, text)\n",
        "    if Dt:\n",
        "      try:\n",
        "        Date = pd.to_datetime(Dt.group(), errors = 'coerce')\n",
        "        if pd.notnull(Date):\n",
        "          return Date.strftime('%Y-%m-%d')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdcAllVcZeaI"
      },
      "source": [
        "## Extracting All Links Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epKTXp5OZJ2m"
      },
      "outputs": [],
      "source": [
        "def AcquiringLinks(soup):\n",
        "  Table = soup.find(\"table\" , {\"class\" : \"wikitable\"})\n",
        "  TableBody = Table.find(\"tbody\")\n",
        "  Links = set() # I used a set to make it unique as in the wikitable the same link for the same movie/person could be repeated alot\n",
        "  MainUrl = \"https://en.wikipedia.org\"\n",
        "  for tr in TableBody.find_all(\"tr\"):\n",
        "    for td in tr.find_all(\"td\"):\n",
        "      for ul in td.find_all(\"ul\"):\n",
        "        for li in ul.find_all(\"li\"):\n",
        "          for a in li.find_all(\"a\"):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "              if href.startswith(\"http\"):\n",
        "                Links.add(href)\n",
        "              else:\n",
        "                Links.add(MainUrl + a[\"href\"])\n",
        "  return Links\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IOHV7eeZlKd"
      },
      "source": [
        "## Checking if Link is a Movie Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def IsLinkMovie(link):\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "        }\n",
        "        response = requests.get(link, headers=headers, timeout=6)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Check if '(film)' is in the page title\n",
        "        page_title = soup.title.string if soup.title else \"\"\n",
        "        if \"(film)\" in page_title.lower():\n",
        "            return True\n",
        "\n",
        "        # Check for infobox\n",
        "        infobox = soup.find(\"table\", class_=\"infobox vevent\")\n",
        "        if not infobox:\n",
        "            return False\n",
        "\n",
        "        movie_identifiers = [\n",
        "            \"Directed by\", \"Produced by\", \"Starring\",\n",
        "            \"Running time\", \"Release date\", \"Production company\",\n",
        "            \"Distributed by\", \"Cinematography\", \"Edited by\"\n",
        "        ]\n",
        "\n",
        "        for th in infobox.find_all(\"th\"):\n",
        "            label = th.get_text(strip=True)\n",
        "            if label in movie_identifiers:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[IsLinkMovie ERROR] {link} → {e}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "uEpToCjuRHp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(IsLinkMovie(\"https://en.wikipedia.org/wiki/The_Devil_Dancer\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz_poaWaMdXo",
        "outputId": "5c3f6916-60c2-4935-b020-384d5c25ddd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myn5yKZmZtAR"
      },
      "source": [
        "## Checking if Link is a Person Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def IsLinkPerson(link):\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "        }\n",
        "        response = requests.get(link, headers=headers, timeout=6)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        infobox = soup.find(\"table\", class_=lambda c: c and (\n",
        "            \"infobox biography vcard\" in c or\n",
        "            \"infobox vcard plainlist\" in c\n",
        "        ))\n",
        "        if not infobox:\n",
        "            #print(f\"[IsLinkPerson] No infobox found for: {link}\")\n",
        "            return False\n",
        "\n",
        "        person_identifiers = [\n",
        "            \"Born\", \"Occupation\", \"Years active\", \"Spouse(s)\", \"Children\",\n",
        "            \"Nationality\", \"Education\", \"Alma mater\", \"Known for\"\n",
        "        ]\n",
        "\n",
        "        for th in infobox.find_all(\"th\"):\n",
        "            label = th.get_text(strip=True)\n",
        "            if label in person_identifiers:\n",
        "                #print(f\"[IsLinkPerson] Confirmed by label '{label}' → {link}\")\n",
        "                return True\n",
        "\n",
        "        #print(f\"[IsLinkPerson] Not a person (no identifiers found): {link}\")\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[IsLinkPerson ERROR] {link} → {e}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "uZNHpmn2YJ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(IsLinkPerson(\"https://en.wikipedia.org/wiki/Annette_Bening\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqbRvUEzYVl8",
        "outputId": "34fc8bf6-f7ed-4b01-d7f7-1eb587baf82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsfrqzHLuuB4"
      },
      "source": [
        "## Checking if Link is a Produciton Com Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrOYfeePuzY0"
      },
      "outputs": [],
      "source": [
        "def IsProdLink(link):\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "        }\n",
        "        response = requests.get(link, headers=headers, timeout=6)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Only look for company-specific infoboxes\n",
        "        infobox = soup.find(\"table\", class_=lambda c: c and (\n",
        "            \"infobox ib-company vcard\" in c or\n",
        "            \"infobox vcard plainlist ib-company\" in c\n",
        "        ))\n",
        "\n",
        "        if not infobox:\n",
        "            #print(f\"[IsProdLink] No infobox found for: {link}\")\n",
        "            return False\n",
        "\n",
        "        prod_identifiers = [\n",
        "            \"Company\", \"Company Type\", \"Formerly\", \"Industry\", \"Founded\",\n",
        "            \"Founders\", \"Headquarters\", \"Products\", \"Parent\"\n",
        "        ]\n",
        "\n",
        "        for th in infobox.find_all(\"th\"):\n",
        "            label = th.get_text(strip=True)\n",
        "            if label in prod_identifiers:\n",
        "                #print(f\"[IsProdLink] Confirmed by label '{label}' → {link}\")\n",
        "                return True\n",
        "\n",
        "        #print(f\"[IsProdLink] Not a production company (no identifiers found): {link}\")\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[IsProdLink ERROR] {link} → {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(IsProdLink(\"https://en.wikipedia.org/wiki/Universal_Pictures\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sma4HA0tYigv",
        "outputId": "ac78e0e9-d3b6-4720-b262-edd3ae68a8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPGLUKzQTivW"
      },
      "source": [
        "## AcademyAwards Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19MSHCr6o1fg"
      },
      "outputs": [],
      "source": [
        "#Extracting Academy Awards Info\n",
        "def AcademyAwardsInfo(soup, Iteration):\n",
        "\n",
        "  Table = soup.find(\"table\" , {\"class\" : \"infobox vevent\"})\n",
        "  TableBody = Table.find(\"tbody\")\n",
        "  #Intializing the Dictionary to store the information in\n",
        "  Data = {}\n",
        "  Data[\"Iteration\"] = Iteration\n",
        "\n",
        "  #Parsing the information card\n",
        "  for tr in TableBody.find_all(\"tr\"):\n",
        "    th = tr.find(\"th\")\n",
        "    td = tr.find(\"td\")\n",
        "    if th and td:\n",
        "\n",
        "      # Removing additional tags\n",
        "      for sup in td.select(\"sup, span\"):\n",
        "        sup.extract()\n",
        "\n",
        "      #Extracting Year\n",
        "      if \"Date\" in th.text:\n",
        "        Date = td.text.strip()\n",
        "        Year = re.search(r'\\d{4}', Date).group()\n",
        "        Data[\"Year\"] = Year\n",
        "\n",
        "      #Extracting Site\n",
        "      if \"Site\" in th.text:\n",
        "        Site = td.text\n",
        "        Data[\"Site\"] = Site\n",
        "\n",
        "      #Extracting the Information of the AcademyAwardsHost Table\n",
        "      if \"Hosted by\" in th.text:\n",
        "        a = td.find_all('a')\n",
        "        for h in a:\n",
        "          Host = h.text.strip()\n",
        "          AcademyAwardsHost.append({\n",
        "              \"IterationNo\" :Iteration,\n",
        "              \"Year\": Year,\n",
        "              \"Hosted by\": Host\n",
        "          })\n",
        "\n",
        "\n",
        "  AcademyAwards.append(Data)\n",
        "  return Data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD5LK6b_Z7yv"
      },
      "source": [
        "## Person Extraction Funtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KoUwFgXaExa"
      },
      "outputs": [],
      "source": [
        "#Building my PersonInformation funciton\n",
        "\n",
        "def PersonInformation(link = None, Name = None, AllPersons = None):\n",
        "# Parsing the person's website\n",
        "  PersonInformation = {}\n",
        "\n",
        "  #The following code will perform if a link was passed in the parameter\n",
        "  if link:\n",
        "\n",
        "\n",
        "    response = requests.get(link)\n",
        "    soup = BeautifulSoup(response.content)\n",
        "    IsDead = False\n",
        "\n",
        "\n",
        "# Getting the person's name from the header\n",
        "    FullName = soup.find(\"h1\", {\"id\" : \"firstHeading\"})\n",
        "    if FullName:\n",
        "      CleanName = re.sub(r\"\\s*\\(.*?\\)\\s*\", \"\", FullName.text.strip())\n",
        "      Name = CleanName.split()\n",
        "      if len(Name) == 1:\n",
        "        FName = Name[0]\n",
        "        PersonInformation['FName'] = FName\n",
        "        PersonInformation['MName'] = None\n",
        "        PersonInformation['LName'] = \"Unknown\"\n",
        "      elif len(Name) == 2:\n",
        "        FName = Name[0]\n",
        "        LName = Name[1]\n",
        "        PersonInformation['FName'] = FName\n",
        "        PersonInformation['MName'] = None\n",
        "        PersonInformation['LName'] = LName\n",
        "\n",
        "      elif len(Name) >=3:\n",
        "        FName = Name[0]\n",
        "        MName = Name[1]\n",
        "        LName = Name[2]\n",
        "        PersonInformation['FName'] = FName\n",
        "        PersonInformation['MName'] = MName\n",
        "        PersonInformation['LName'] = LName\n",
        "\n",
        "# Parsing the information card\n",
        "    Table = soup.find(\"table\" , {\"class\": re.compile(r\"infobox.*vcard|infobox.*plainlist\")})\n",
        "    if Table:\n",
        "      TableBody = Table.find(\"tbody\")\n",
        "\n",
        "      for tr in TableBody.find_all(\"tr\"):\n",
        "          th = tr.find(\"th\")\n",
        "          td = tr.find(\"td\")\n",
        "          if th and td:\n",
        "            # Removing additional tags\n",
        "            for sup in td.select(\"sup, span\"):\n",
        "              sup.extract()\n",
        "\n",
        "            #Extracting BirthDate and Country of Birth\n",
        "            if \"Born\" in th.text:\n",
        "              BirthDate = td.text.strip()\n",
        "              BDate = ExtractDate(BirthDate)\n",
        "              PersonInformation[\"BirthDate\"] = BDate\n",
        "              BirthPlace = td.find(\"div\", {\"class\": \"birthplace\"})\n",
        "              if BirthPlace:\n",
        "                a = BirthPlace.find(\"a\")\n",
        "                if a:\n",
        "                  CountryOfBirth = a.text.strip()\n",
        "                  PersonInformation[\"CountryOfBirth\"] = CountryOfBirth\n",
        "                elif not a:\n",
        "                  CountryOfBirth = BirthPlace.text.strip()\n",
        "                  PersonInformation[\"CountryOfBirth\"] = CountryOfBirth\n",
        "                else:\n",
        "                  PersonInformation[\"CountryOfBirth\"] = 'Unknown'\n",
        "\n",
        "            #Extracting DeathDate\n",
        "            if \"Died\" in th.text:\n",
        "              IsDead = True\n",
        "              DeathDate = td.text.strip()\n",
        "              DDate = ExtractDate(DeathDate)\n",
        "              PersonInformation[\"DeathDate\"] = DDate\n",
        "\n",
        "          else:\n",
        "            continue # There was an error as in the first row tr it had th only and not td so the code inside the if condition wouldn't excute\n",
        "\n",
        "          if IsDead == False:\n",
        "            PersonInformation[\"DeathDate\"] = None\n",
        "\n",
        "#The following code will perform if Name was passed in the parameter\n",
        "  elif Name:\n",
        "    PName = Name.strip().split()\n",
        "    if len(PName) == 2:\n",
        "      FName = PName[0]\n",
        "      LName = PName[1]\n",
        "      PersonInformation['FName'] = FName\n",
        "      PersonInformation['MName'] = None\n",
        "      PersonInformation['LName'] = LName\n",
        "    elif len(PName) == 3:\n",
        "      FName = PName[0]\n",
        "      MName = PName[1]\n",
        "      LName = PName[2]\n",
        "      PersonInformation['FName'] = FName\n",
        "      PersonInformation['MName'] = MName\n",
        "      PersonInformation['LName'] = LName\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "    PersonInformation[\"BirthDate\"] = None\n",
        "    PersonInformation[\"CountryOfBirth\"] = None\n",
        "    PersonInformation[\"DeathDate\"] = None\n",
        "\n",
        "\n",
        "  if PersonInformation.get(\"FName\"):\n",
        "    AllPersons.append(PersonInformation)\n",
        "\n",
        "  return PersonInformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpbcCaQUa0TE"
      },
      "source": [
        "## Movie Extraction Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEgJBm1ka98R"
      },
      "outputs": [],
      "source": [
        "def MovieInformation(link):\n",
        "  ProdComp = []\n",
        "  Movie = {}\n",
        "  ReleaseDt = \"\"\n",
        "  t = \"\"\n",
        "\n",
        "  # Parsing the web pages\n",
        "  response = requests.get(link)\n",
        "  soup = BeautifulSoup(response.content)\n",
        "  Table = soup.find(\"table\" , {\"class\" : \"infobox vevent\"})\n",
        "  TableBody = Table.find(\"tbody\")\n",
        "\n",
        "  Title = soup.find(\"th\" , {\"class\": \"infobox-above summary\"})\n",
        "  if Title:\n",
        "    t = Title.text.strip()\n",
        "    Movie[\"Title\"] = t\n",
        "  else:\n",
        "    t = None\n",
        "\n",
        "  for tr in TableBody.find_all(\"tr\"):\n",
        "    #Parsing the title\n",
        "\n",
        "    # Getting all the other information to store in  my movie table\n",
        "    th = tr.find(\"th\")\n",
        "    td = tr.find(\"td\")\n",
        "    if th and td:\n",
        "\n",
        "      #Removing additional tags\n",
        "      for sup in td.select(\"sup, span\"):\n",
        "        sup.extract()\n",
        "\n",
        "      #Parsing the Release date\n",
        "      if  \"Release date\" in th.text:\n",
        "        ul = td.find(\"ul\")\n",
        "        if ul:\n",
        "          li = ul.find(\"li\")\n",
        "          if li:\n",
        "            Date = li.text.strip().replace('\\xa0', ' ')\n",
        "            FullDate = ExtractDate(Date) # Had to extracrt yyyy-mm-dd in order to matche the date data type of mysql\n",
        "            Movie[\"Release Date\"] = FullDate\n",
        "            ReleaseDt = FullDate\n",
        "        else:\n",
        "          Date = td.text.strip().replace('\\xa0', ' ')\n",
        "          FullDate = ExtractDate(Date) # Had to extracrt yyyy-mm-dd in order to matche the date data type of mysql\n",
        "          Movie[\"Release Date\"] = FullDate\n",
        "          ReleaseDt = FullDate\n",
        "\n",
        "      #Extracting Country\n",
        "      if \"Country\" in th.text or \"Countries\" in th.text:\n",
        "        ul = td.find(\"ul\")\n",
        "        if ul:\n",
        "          li = ul.find(\"li\")\n",
        "          if li:\n",
        "            Country = li.text.strip()\n",
        "            Movie[\"Country\"] = Country\n",
        "        else:\n",
        "          Country = td.text.strip()\n",
        "          Movie[\"Country\"] = Country\n",
        "\n",
        "      #Extracting Language\n",
        "      if \"Language\" in th.text:\n",
        "        Language = td.text.strip()\n",
        "        Movie[\"Language\"] = Language\n",
        "\n",
        "      #Getting the Running time of the movie\n",
        "      if \"Running time\" in th.text:\n",
        "        time = re.search(r'\\d+', td.text)\n",
        "        if time:\n",
        "          Movie['Runtime'] = int(time.group())\n",
        "        else:\n",
        "          Movie['Runtime'] = None\n",
        "\n",
        "      #Acquiring the production company or companies information for my MovieProductionCompany table\n",
        "      if re.search(r'Production.*company|Distributed by|Production.*companies', th.text):\n",
        "        ul = td.find(\"ul\")\n",
        "        if ul:\n",
        "          for li in ul.find_all(\"li\"):\n",
        "            a = li.find(\"a\")\n",
        "            if a:\n",
        "              Comp = a.text.strip()\n",
        "              ProdComp.append(Comp)\n",
        "        else:\n",
        "          for a in td.find_all(\"a\"):\n",
        "            Comp = a.text.strip()\n",
        "            ProdComp.append(Comp)\n",
        "\n",
        "  for tr in TableBody.find_all(\"tr\"):\n",
        "    th = tr.find(\"th\")\n",
        "    td = tr.find(\"td\")\n",
        "    if th and td:\n",
        "      #Removing additional tags\n",
        "      for sup in td.select(\"sup, span\"):\n",
        "        sup.extract()\n",
        "      # Acquiring the Roles and placing the information in my Persons Table\n",
        "      for key, value in Roles.items():\n",
        "        if key in th.text:\n",
        "          if td.find_all(\"ul\"):\n",
        "            for li in td.find_all(\"li\"):\n",
        "              links = li.find_all(\"a\")\n",
        "              if links:\n",
        "                for a in links:\n",
        "                  href = a.get(\"href\")\n",
        "                  if href:\n",
        "                    if href.startswith(\"http\"):\n",
        "                      link = href\n",
        "                    else:\n",
        "                      link = \"https://en.wikipedia.org\" + href\n",
        "                  Person = PersonInformation(link=link, AllPersons=AllPersons)\n",
        "                  if Person:\n",
        "                    WorksAt.append({\n",
        "                      \"FName\": Person[\"FName\"],\n",
        "                      \"MName\": Person[\"MName\"],\n",
        "                      \"LName\": Person[\"LName\"],\n",
        "                      \"Title\": t,\n",
        "                      \"ReleaseDate\": ReleaseDt,\n",
        "                      \"Role\": value\n",
        "                  })\n",
        "              else:\n",
        "                name = li.text.strip()\n",
        "                Person = PersonInformation(Name=name, AllPersons=AllPersons)\n",
        "                if Person:\n",
        "                  WorksAt.append({\n",
        "                      \"FName\": Person[\"FName\"],\n",
        "                      \"MName\": Person[\"MName\"],\n",
        "                      \"LName\": Person[\"LName\"],\n",
        "                      \"Title\": t,\n",
        "                      \"ReleaseDate\": ReleaseDt,\n",
        "                      \"Role\": value\n",
        "                  })\n",
        "          else:\n",
        "            links = td.find_all(\"a\")\n",
        "            if links:\n",
        "                for a in links:\n",
        "                  href = a.get(\"href\")\n",
        "                  if href:\n",
        "                    if href.startswith(\"http\"):\n",
        "                      link = href\n",
        "                    else:\n",
        "                      link = \"https://en.wikipedia.org\" + href\n",
        "                  Person = PersonInformation(link=link, AllPersons=AllPersons)\n",
        "                  if Person:\n",
        "                    WorksAt.append({\n",
        "                      \"FName\": Person[\"FName\"],\n",
        "                      \"MName\": Person[\"MName\"],\n",
        "                      \"LName\": Person[\"LName\"],\n",
        "                      \"Title\": t,\n",
        "                      \"ReleaseDate\": ReleaseDt,\n",
        "                      \"Role\": value\n",
        "                    })\n",
        "            else:\n",
        "              name = td.text.strip()\n",
        "              Person = PersonInformation(Name=name, AllPersons=AllPersons)\n",
        "              if Person:\n",
        "                WorksAt.append({\n",
        "                      \"FName\": Person['FName'],\n",
        "                      \"MName\": Person[\"MName\"],\n",
        "                      \"LName\": Person[\"LName\"],\n",
        "                      \"Title\": t,\n",
        "                      \"ReleaseDate\": ReleaseDt,\n",
        "                      \"Role\": value\n",
        "                  })\n",
        "\n",
        "  for c in ProdComp:\n",
        "    MovieProdComp.append({\n",
        "            \"Title\": t,\n",
        "            \"ReleaseDate\": ReleaseDt,\n",
        "            \"ProductionCompany\": c\n",
        "            })\n",
        "\n",
        "\n",
        "  Movies.append(Movie)\n",
        "\n",
        "  return {\"Movie\": Movie}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGW9AKHvb8hY"
      },
      "source": [
        "## Nomination Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpXQofCYcAtM"
      },
      "outputs": [],
      "source": [
        "def NominationInformation(soup, IterationNo, Year):\n",
        "  Table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
        "  TableBody = Table.find(\"tbody\")\n",
        "  Category = \" \"\n",
        "\n",
        "\n",
        "  for tr in TableBody.find_all(\"tr\"):\n",
        "    for td in tr.find_all(\"td\"):\n",
        "      d = td.find(\"div\")\n",
        "      if d:\n",
        "        a = d.find(\"a\")\n",
        "        if a:\n",
        "          Category = a.text.strip()\n",
        "        else:\n",
        "          Category = d.text.strip()\n",
        "      for OuterUl in td.find_all(\"ul\", recursive = False):\n",
        "        for li in OuterUl.find_all(\"li\", recursive = False):\n",
        "          ExtractNomination(li, Category, IterationNo, Year, IsGranted = True)\n",
        "\n",
        "          InnerUl = li.find(\"ul\")\n",
        "          if InnerUl:\n",
        "            for NomineeLi in InnerUl.find_all(\"li\", recursive = False):\n",
        "                ExtractNomination(NomineeLi, Category, IterationNo, Year, IsGranted = False)\n",
        "\n",
        "\n",
        "\n",
        "def ExtractNomination(li, Category, IterationNo, Year, IsGranted):\n",
        "\n",
        "  Nom = {\n",
        "      \"IterationNo\": IterationNo,\n",
        "      \"Year\": Year,\n",
        "      \"Category\": Category,\n",
        "      \"IsGranted\": IsGranted,\n",
        "      \"MovieTitle\": None,\n",
        "      \"ReleaseDate\": None,\n",
        "      \"FName\": None,\n",
        "      \"MName\": None,\n",
        "      \"LName\": None\n",
        "  }\n",
        "\n",
        "  MovieFound = False\n",
        "  PersonFound = False\n",
        "\n",
        "  Links = li.find_all(\"a\")\n",
        "  for a in Links:\n",
        "    href = a.get(\"href\")\n",
        "    if href:\n",
        "      if href.startswith(\"http\"):\n",
        "          Link = href\n",
        "      else:\n",
        "          Link = \"https://en.wikipedia.org\" + href\n",
        "    if IsProdLink(Link):\n",
        "      continue\n",
        "    if not MovieFound and IsLinkMovie(Link):\n",
        "      Movie = MovieInformation(Link)\n",
        "      if Movie:\n",
        "        Nom[\"MovieTitle\"] = Movie[\"Movie\"].get(\"Title\")\n",
        "        Nom[\"ReleaseDate\"] = Movie[\"Movie\"].get(\"Release Date\")\n",
        "        MovieFound = True\n",
        "    elif not PersonFound and IsLinkPerson(Link):\n",
        "      Person = PersonInformation(Link, AllPersons = AllPersons)\n",
        "      if Person:\n",
        "          Nom[\"FName\"] = Person[\"FName\"]\n",
        "          Nom[\"MName\"] = Person[\"MName\"]\n",
        "          Nom[\"LName\"] = Person[\"LName\"]\n",
        "          PersonFound = True\n",
        "\n",
        "  if not Links:\n",
        "    Name = li.text.strip()\n",
        "    if Name:\n",
        "      P = PersonInformation(Name = Name , AllPersons = AllPersons)\n",
        "      if P:\n",
        "        Nom[\"FName\"] = P[\"FName\"]\n",
        "        Nom[\"MName\"] = P[\"MName\"]\n",
        "        Nom[\"LName\"] = P[\"LName\"]\n",
        "\n",
        "  Nomination.append(Nom)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "SuffixNum = '1st'\n",
        "URL = f\"https://en.wikipedia.org/wiki/{SuffixNum}_Academy_Awards\"\n",
        "print(f\"{SuffixNum} Academy Awards\")\n",
        "try:\n",
        "  response = requests.get(URL)\n",
        "  soup = BeautifulSoup(response.content)\n",
        "  A = AcademyAwardsInfo(soup, i)\n",
        "  NominationInformation(soup, A[\"Iteration\"], A[\"Year\"])\n",
        "  time.sleep(1.5)\n",
        "\n",
        "except Exception as E:\n",
        "  print(f\"Failed To Extract Iteration {i}: {E}\")\n",
        "  time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPiXydpwfvTH",
        "outputId": "fdf27940-e4d1-48ef-eaa5-53a95518a7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st Academy Awards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynvrm2hicEDT"
      },
      "source": [
        "# Webscraping the 96 iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb8dggQOcPcw",
        "outputId": "b6db8761-427c-4afb-8520-3433ae1b5a52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1st Academy Awards\n",
            "2nd Academy Awards\n",
            "3rd Academy Awards\n",
            "4th Academy Awards\n",
            "5th Academy Awards\n",
            "6th Academy Awards\n",
            "7th Academy Awards\n",
            "8th Academy Awards\n",
            "9th Academy Awards\n",
            "10th Academy Awards\n",
            "11th Academy Awards\n",
            "12th Academy Awards\n",
            "13th Academy Awards\n",
            "14th Academy Awards\n",
            "15th Academy Awards\n",
            "16th Academy Awards\n",
            "17th Academy Awards\n",
            "18th Academy Awards\n",
            "19th Academy Awards\n",
            "20th Academy Awards\n",
            "21st Academy Awards\n",
            "22nd Academy Awards\n",
            "23rd Academy Awards\n",
            "24th Academy Awards\n",
            "25th Academy Awards\n",
            "26th Academy Awards\n",
            "27th Academy Awards\n",
            "28th Academy Awards\n",
            "29th Academy Awards\n",
            "30th Academy Awards\n",
            "31st Academy Awards\n",
            "32nd Academy Awards\n",
            "33rd Academy Awards\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,97):\n",
        "  SuffixNum = number.ordinal(i)\n",
        "  URL = f\"https://en.wikipedia.org/wiki/{SuffixNum}_Academy_Awards\"\n",
        "  print(f\"{SuffixNum} Academy Awards\")\n",
        "  try:\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "    })\n",
        "    response = session.get(URL, timeout=10)\n",
        "    soup = BeautifulSoup(response.content)\n",
        "    A = AcademyAwardsInfo(soup, i)\n",
        "    NominationInformation(soup, A[\"Iteration\"], A[\"Year\"])\n",
        "    time.sleep(1.25)\n",
        "\n",
        "  except Exception as E:\n",
        "    print(f\"Failed To Extract Iteration {i}: {E}\")\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9w1mIIj60rj"
      },
      "source": [
        "## Storing Values Into CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjpzHraj640B"
      },
      "outputs": [],
      "source": [
        "PersonDf = pd.DataFrame(AllPersons)\n",
        "PersonDf.to_csv(\"Person.csv\", index = False)\n",
        "\n",
        "WorksAtDf = pd.DataFrame(WorksAt)\n",
        "WorksAtDf.to_csv(\"WorksAt.csv\", index = False)\n",
        "\n",
        "MovieDf = pd.DataFrame(Movies)\n",
        "MovieDf.to_csv(\"Movie.csv\", index = False)\n",
        "\n",
        "MovieProdCompDf = pd.DataFrame(MovieProdComp)\n",
        "MovieProdCompDf.to_csv(\"MovieProdComp.csv\", index = False)\n",
        "\n",
        "AcademyAwardsDf = pd.DataFrame(AcademyAwards)\n",
        "AcademyAwardsDf.to_csv(\"AcademyAwards.csv\", index = False)\n",
        "\n",
        "AcademyAwardsHostDf = pd.DataFrame(AcademyAwardsHost)\n",
        "AcademyAwardsHostDf.to_csv(\"AcademyAwardsHost.csv\", index = False)\n",
        "\n",
        "NominationDf = pd.DataFrame(Nomination)\n",
        "NominationDf.to_csv(\"Nomination.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ISQ_P4m3HPg"
      },
      "source": [
        "# Sample Test of User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r65n2TDZykbl",
        "outputId": "58aefa85-49c0-4a0a-eadb-a381b11eb605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (37.1.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "[{'EmailAddress': 'thogan@example.com', 'UserName': 'tmartinez', 'BirthDate': datetime.date(1975, 7, 17), 'Age': 50, 'Gender': 'M', 'CountryOfBirth': 'British Indian Ocean Territory (Chagos Archipelago)'}, {'EmailAddress': 'hdurham@example.org', 'UserName': 'brittneyweber', 'BirthDate': datetime.date(1963, 9, 13), 'Age': 62, 'Gender': 'M', 'CountryOfBirth': 'Tonga'}, {'EmailAddress': 'johnpeterson@example.org', 'UserName': 'craig10', 'BirthDate': datetime.date(1996, 7, 2), 'Age': 29, 'Gender': 'M', 'CountryOfBirth': 'Bouvet Island (Bouvetoya)'}, {'EmailAddress': 'frank40@example.org', 'UserName': 'traceywhitney', 'BirthDate': datetime.date(1965, 12, 28), 'Age': 60, 'Gender': 'F', 'CountryOfBirth': 'Mauritius'}, {'EmailAddress': 'zrodriguez@example.org', 'UserName': 'zbrown', 'BirthDate': datetime.date(1946, 9, 18), 'Age': 79, 'Gender': 'M', 'CountryOfBirth': 'Guinea'}, {'EmailAddress': 'stricklanddaniel@example.com', 'UserName': 'olivingston', 'BirthDate': datetime.date(2007, 3, 20), 'Age': 18, 'Gender': 'F', 'CountryOfBirth': 'Turkey'}, {'EmailAddress': 'brookstroy@example.com', 'UserName': 'ericschultz', 'BirthDate': datetime.date(1992, 9, 8), 'Age': 33, 'Gender': 'F', 'CountryOfBirth': 'Turkmenistan'}, {'EmailAddress': 'ksingh@example.net', 'UserName': 'douglasking', 'BirthDate': datetime.date(1990, 12, 22), 'Age': 35, 'Gender': 'M', 'CountryOfBirth': 'Bolivia'}, {'EmailAddress': 'qsmith@example.org', 'UserName': 'brandonbrown', 'BirthDate': datetime.date(1963, 1, 14), 'Age': 62, 'Gender': 'M', 'CountryOfBirth': 'Mauritius'}, {'EmailAddress': 'igraham@example.net', 'UserName': 'ralph45', 'BirthDate': datetime.date(1947, 12, 27), 'Age': 78, 'Gender': 'F', 'CountryOfBirth': 'Guinea'}]\n"
          ]
        }
      ],
      "source": [
        "!pip install faker\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "Fake = Faker()\n",
        "\n",
        "def GenerateUsers(n):\n",
        "  User = []\n",
        "  for i in range(1, (n+1)):\n",
        "    EmailAddress = Fake.email()\n",
        "    UserName = Fake.user_name()\n",
        "    BirthDate = Fake.date_of_birth(minimum_age = 18, maximum_age = 80)\n",
        "    Age = datetime.now().year - BirthDate.year\n",
        "    Gender = random.choice(['M' , 'F'])\n",
        "    CountryOfBirth = Fake.country()\n",
        "\n",
        "    User.append({\n",
        "        \"EmailAddress\": EmailAddress,\n",
        "        \"UserName\": UserName,\n",
        "        \"BirthDate\" : BirthDate ,\n",
        "        \"Age\" : Age ,\n",
        "        \"Gender\" : Gender ,\n",
        "        \"CountryOfBirth\" : CountryOfBirth\n",
        "    })\n",
        "\n",
        "    i = i + 1\n",
        "\n",
        "  return User\n",
        "\n",
        "\n",
        "UserRandom = GenerateUsers(10)\n",
        "print(UserRandom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyCoPWRL5DAX",
        "outputId": "75355dd8-df3a-4017-d8e6-e282414483c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'EmailAddress': 'stricklanddaniel@example.com', 'IterationNo': 72, 'Year': 1958, 'Category': 'Best Cinematography', 'MovieTitle': 'The Crowd', 'ReleaseDate': '1927-05-06', 'FName': 'George', 'MName': None, 'LName': 'Robinson'}, {'EmailAddress': 'brookstroy@example.com', 'IterationNo': 75, 'Year': 1987, 'Category': 'Best Actress', 'MovieTitle': 'Speedy', 'ReleaseDate': '1928-01-07', 'FName': 'Otto', 'MName': 'Hermann', 'LName': 'Arlen'}, {'EmailAddress': 'brookstroy@example.com', 'IterationNo': 87, 'Year': 1976, 'Category': 'Best Cinematography', 'MovieTitle': 'The Noose', 'ReleaseDate': '1928-01-29', 'FName': 'Samuel', 'MName': None, 'LName': 'Korda'}, {'EmailAddress': 'zrodriguez@example.org', 'IterationNo': 62, 'Year': 1939, 'Category': 'Best Production', 'MovieTitle': 'Sunrise: A Song of Two Humans', 'ReleaseDate': '1927-11-04', 'FName': 'Herbert', 'MName': None, 'LName': 'Chaplin'}, {'EmailAddress': 'stricklanddaniel@example.com', 'IterationNo': 12, 'Year': 1952, 'Category': 'Best Production', 'MovieTitle': 'A Ship Comes In', 'ReleaseDate': '1928-02-28', 'FName': 'Karl', 'MName': None, 'LName': 'Barnes'}, {'EmailAddress': 'zrodriguez@example.org', 'IterationNo': 96, 'Year': 2015, 'Category': 'Best Writing', 'MovieTitle': 'Tempest', 'ReleaseDate': '1928-05-27', 'FName': 'Karl', 'MName': None, 'LName': 'Robinson'}, {'EmailAddress': 'thogan@example.com', 'IterationNo': 41, 'Year': 2004, 'Category': 'Best Cinematography', 'MovieTitle': '7th Heaven', 'ReleaseDate': '1927-08-20', 'FName': 'Casey', 'MName': 'Stepan', 'LName': 'Chaplin'}, {'EmailAddress': 'brookstroy@example.com', 'IterationNo': 20, 'Year': 1981, 'Category': 'Best Actor', 'MovieTitle': 'Chang: A Drama of the Wilderness', 'ReleaseDate': '1928-01-22', 'FName': 'Clara', 'MName': None, 'LName': 'Lasky'}, {'EmailAddress': 'johnpeterson@example.org', 'IterationNo': 74, 'Year': 1964, 'Category': 'Best Picture', 'MovieTitle': 'The Last Command', 'ReleaseDate': '1928-01-04', 'FName': 'George', 'MName': None, 'LName': 'Milestone'}, {'EmailAddress': 'hdurham@example.org', 'IterationNo': 56, 'Year': 1939, 'Category': 'Best Actor', 'MovieTitle': 'Sunrise: A Song of Two Humans', 'ReleaseDate': '1928-05-27', 'FName': 'Louis', 'MName': None, 'LName': 'Brent'}]\n"
          ]
        }
      ],
      "source": [
        "def GenerateUserNominations(n, Users):\n",
        "  Categories = [\"Best Actor\", \"Best Cinematography\", \"Best Writing\", \"Best Picture\", \"Best Actress\", \"Best Director\", \"Best Production\"]\n",
        "  UserNom = []\n",
        "  for i in range(1, (n+1)):\n",
        "    User = random.choice(Users)\n",
        "    Year = random.randint(1930,2024)\n",
        "    Iteration = random.randint(1,97)\n",
        "    MovieTitle = random.choice(Movies)[\"Title\"]\n",
        "    ReleaseDate = random.choice(Movies)[\"Release Date\"]\n",
        "    FName = random.choice(AllPersons)[\"FName\"]\n",
        "    MName = random.choice(AllPersons)[\"MName\"]\n",
        "    LName = random.choice(AllPersons)[\"LName\"]\n",
        "    Category = random.choice(Categories)\n",
        "\n",
        "    UserNom.append({\n",
        "        \"EmailAddress\": User[\"EmailAddress\"],\n",
        "        \"IterationNo\": Iteration,\n",
        "        \"Year\": Year,\n",
        "        \"Category\": Category,\n",
        "        \"MovieTitle\": MovieTitle,\n",
        "        \"ReleaseDate\": ReleaseDate,\n",
        "        \"FName\": FName,\n",
        "        \"MName\": MName,\n",
        "        \"LName\": LName\n",
        "    })\n",
        "\n",
        "    i = i + 1\n",
        "\n",
        "  return UserNom\n",
        "\n",
        "\n",
        "UserNomRandom = GenerateUserNominations(10, UserRandom)\n",
        "print(UserNomRandom)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a8Yo_tH8ZSA"
      },
      "outputs": [],
      "source": [
        "UserDf = pd.DataFrame(UserRandom)\n",
        "UserDf.to_csv(\"User.csv\", index = False)\n",
        "\n",
        "UserNomDf = pd.DataFrame(UserNomRandom)\n",
        "UserNomDf.to_csv(\"UserNomination.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "        00:37:35\tDrop table Oscars.User\tError Code: 3730. Cannot drop table 'user' referenced by a foreign key constraint 'usernomination_ibfk_3' on table 'usernomination'.\t0.000 sec\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "_MerLlD0zRC5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nmZR4uhnTYXV",
        "fdcAllVcZeaI",
        "7IOHV7eeZlKd",
        "QPGLUKzQTivW",
        "xpbcCaQUa0TE",
        "0ISQ_P4m3HPg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}